一.为什么卷积神经网络中卷积核边长一般为奇数？     
　　答：1.如果卷积核边长为偶数，根据padding公式，当采用SAME-Padding时会产生不对称填充，      
　　　　　而边长为奇数时，会进行自然填充。      
　　　　2.当卷积核边长为奇数时，卷积核具有一个中心点，方便进行中心点的定位。   
    
二.特征可视化带来的启示？   
　　答：1.一组特征的输入特征（通过重构获得）将刺激卷积网产生一个固定的输出特征。这些重构     
　　　　　特征可能只包含判别纹理结构的能力因此当输入存在一定畸变时，网络的输出结果保持不变。     
　　　　2.卷积网络不同层所展示的特征是不同的。底层可能检测物体的颜色、边缘与轮廓，中层主要展     
　　　　　示相似的纹理，高层主要开始体现类与类之间的差异   
　　　　3.特征在训练过程中的演化：在训练过程中由特定输出特征反向卷积，所获得的最强重构输入特   
　　　　　征展示了变化过程。当输入图片中的最强刺激源发生变化时对应的输出特征轮廓发生跳变，经过   
　　　　　一定次数的迭代，底层特征趋于稳定，但更高层的特征则需要更多的迭代收敛，一般需要（40-50周期）    
　　　　4.特征不变性：在底层，很小的微变都会导致输出特征变化明显，越往高层走，平移和尺度变化    
　　　　　对最终的结果影响越小。总体来说，卷积网络无法对旋转操作产生不变性，除非物体具有很强的对称性。    
　　　　5.卷积核边长过大会混杂大量高频和低频信号，跨度过大会产生混乱无用的特征。  
　　　　6.卷积网络中层对部件级的相关性更为关注，高层则会关注更高层的信息。以狗为例，中层会关    
　　　　　注眼睛鼻子的相关性，而高层则更会去区分狗的种类   
　　　　7.在5卷积层+2全连接层的网络中，删除全连接层或2个卷积隐含层，错误率也只有轻微的上升。   
　　　　　当所有的卷积隐含层都被去掉了，模型能力大幅下降。这说明模型的深度与分类效果密切相关，   
　　　　　深度越大，效果越好。改变全连接层的节点个数对分类性能印象不大，扩大中间卷积层的节点个   
　　　　　数对训练效果有提高，但同时也加大了全连接层出现过拟合的可能。
     
三.Python中Iterator和Iterable的区别？     
　　答：1.Pyhon中 list，truple，str，dict这些都可以被迭代，但他们并不是迭代器。       
　　　　　因为和迭代器相比有一个很大的不同，list/truple/map/dict这些数据的大小是确定的，也就是说有多少事可知的。            
　　　　　但迭代器不是，迭代器不知道要执行多少次，所以可以理解为不知道有多少个元素，每调用一次next()，        
　　　　　就会往下走一步，是惰性的。      
　　　　　因为Python的Iterator对象表示的是一个数据流，Iterator对象可以被next()函数调用并不断返回下一个数据，    
　　　　　直到没有数据时抛出StopIteration错误。可以把这个数据流看做是一个有序序列，但我们却不能提前知道序列的   
　　　　　长度，只能不断通过next()函数实现按需计算下一个数据，所以Iterator的计算是惰性的，只有在需要返回下一个   
　　　　　数据时它才会计算。Iterator甚至可以表示一个无限大的数据流，例如全体自然数。而使用list是永远不可能存储    
　　　　　全体自然数的。   
　　　　2.凡是可以for循环的，都是Iterable     
　　　　　凡是可以next()的，都是Iterator    
　　　　3.集合数据类型如list，truple，dict，str，都是Itrable不是Iterator，但可以通过iter()函数获得一个Iterator对象    
　　　　　Python中的for循环就是通过next实现的    
        
